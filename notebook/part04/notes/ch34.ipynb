{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b2e607924b7d11",
   "metadata": {},
   "source": [
    "# 第34章 强化学习简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3aeb2",
   "metadata": {},
   "source": [
    "## 习题34.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd16750",
   "metadata": {},
   "source": [
    "&emsp;&emsp;写出奖励、回报、价值（状态价值）的定义，比较三者之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24401c0",
   "metadata": {},
   "source": [
    "**解答：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f336d",
   "metadata": {},
   "source": [
    "**解答思路：** 比较Reward、Return和State Value三者关系的关键是从**局部反馈**逐步过渡到**长期评估**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf0c86",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "**第一步：理解奖励属于是环境与智能体交互每一步的局部反馈**\n",
    "\n",
    "每一步环境的即时反馈只有一个：\n",
    "\n",
    "$$R_{t+1}= R(s_t, a_t) \\tag{34.1}$$\n",
    "\n",
    "表示 $R_t$ 是智能体在时间步 $t$ 时，执行动作 $a_t$ 后从环境获得的奖励。\n",
    "\n",
    "**第二步：理解回报是考虑从时间 $t$ 到步终止的局部累加奖励总和**\n",
    "\n",
    "在一个轨迹中：\n",
    "\n",
    "- 对于**有限时域马尔可夫决策过程（Finite-Horizon MDP）**，决策过程在有限步后终止的状态转移和奖励通常仍然是随机的。因此，从时间步 $t$ 开始的未来奖励总和可以表示为：\n",
    "\n",
    "$$G_t= R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}=\\sum_{k=0}^k \\gamma^k R_{t+k+1}, k=0,1,...,n \\tag{34.2}$$\n",
    "\n",
    "- 对于无限马尔可夫决策过程（IMDP），由于时间步 $t$ 是无限的还是可以计算状态 $s_t$ 到未来总奖励，即从状态 $s_t$ 开始到未来的奖励期望，但是关键是必须保证 $\\gamma＜1$ ，否则未来奖励总和将是无限大无法收敛状态是没有办法计算相应的 $G_t$ ：\n",
    "\n",
    "$$G_t= R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}=\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}, k=0,1,... \\tag{34.3}$$\n",
    "\n",
    "其中 $\\gamma$ 是折扣因子表示随着时间的推移对应的动作、状态对未来总奖励的重要性越来越小。\n",
    "\n",
    "**第三步：理解状态价值是考虑从状态 $s_t$ 开始的未来总奖励的期望平均值**\n",
    "\n",
    "$$V_{Π}(s)= E[G_t | S_t=s] = E[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots | S_t=s] \\tag{34.4}$$\n",
    "\n",
    "\n",
    "表示在一个策略 $\\pi$ 下，从状态 $s_t$ 出发，状态价值 $V_\\pi(s)$ 是在所有可能的轨迹对未来回报 $G_t$ 的期望表示为 $V_\\pi(s)$ ，也就是说，状态价值衡量智能体在策略 $\\pi$ 下从当前状态开始到未来可能回报的平均水平，其中 $\\mathbb{E}[\\cdot]$ 表示期望运算符。\n",
    "\n",
    "**三者的关系总结：**\n",
    "\n",
    "- 奖励（Reward）是环境与智能体在每一步交互后给出的即时反馈信号，用于衡量当前状态–动作对的局部效果。\n",
    "- 回报（Return）是从轨迹的某一时刻 $t $ 开始，对未来所有奖励（通常带折扣因子 $\\gamma$ ）进行累加得到的总收益，是从“单步局部反馈”到“长期累计收益”的时间展开结果。\n",
    "- 状态价值（State Value）是在给定策略 $\\pi$ 下，从状态 $s_t$ 出发所能获得的未来回报 $G_t$ 的期望值。状态价值不是对单步奖励取平均，而是对未来回报进行期望运算，从而得到对该状态长期收益的稳定评估。\n",
    "\n",
    "因此用一句话概括为——每一步的奖励经过时间累积形成回报，而回报经过期望运算形成状态价值：\n",
    "\n",
    "$$Reward -> Return -> Value \\tag{34.5}$$\n",
    "\n",
    "> 其中，轨迹（Trajectory）是指从初始状态开始，在某一策略作用下，依次经历状态、动作与奖励所形成的序列；策略（Policy）是指智能体在每个状态下选择动作的规则或概率分布。**两者的关系是：策略决定轨迹的分布，而多条轨迹的统计结果体现了该策略的行为特征与长期效果**。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca44cd",
   "metadata": {},
   "source": [
    "## 习题34.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394480f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;证明状态-动作的转移概率分布也具有马尔可夫性(34.3)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab9ea7",
   "metadata": {},
   "source": [
    "**解答：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14093b3c",
   "metadata": {},
   "source": [
    "**解答思路：**\n",
    "1. 明确马尔可夫性质的本质\n",
    "2. 思考状态-动作对的转移概率分布表达方式\n",
    "3. 应用MDP的马尔可夫性与策略定义\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d30d87",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "**第一步：借助链式法则拆分原始的转移概率方程（34.6）**\n",
    "\n",
    "$$P(S_{t+1},A_{t+1}|S_0, A_0, S_1, A_1, \\cdots, S_{t}, A_{t}) \\tag{34.6} $$\n",
    "\n",
    "由于条件概率满足链式法则，借助这个法则拆分（34.6）：\n",
    "\n",
    "$$P(S_{t+1},A_{t+1}|S_0, A_0, S_1, A_1, \\cdots, S_{t}, A_{t})=P(S_{t+1}|S_0, A_0,..., S_{t}, A_{t}) \\times P(A_{t+1}|S_0, A_0,..., S_{t}, A_{t},S_{t+1}) \\tag{34.7} $$\n",
    "\n",
    ">链式法则为 $P(X,Y|Z)=P(X|Z)P(Y|X,Z)$ ， $Y$ 可能依赖于 $X$ 。\n",
    "\n",
    "根据（34.7），我们可以得到两个条件概率式子：\n",
    "\n",
    "$$P(S_{t+1}|S_0, A_0,..., S_{t}, A_{t}) \\tag{34.8} $$\n",
    "\n",
    "$$P(A_{t+1}|S_0, A_0,..., S_{t}, A_{t},S_{t+1}) \\tag{34.9} $$\n",
    "\n",
    "**第二步：应用MDP的马尔可夫性**\n",
    "\n",
    "在原书中提到 $P(s'|s,a)$ 有马尔可夫性，以及**马尔可夫策略 $Π(s'|a')$ 也就是动作选择只依赖当前状态**，即可以把式子（34.8）、（34.9）变形为：\n",
    "\n",
    "$$P(S_{t+1}|S_0, A_0,..., S_{t}, A_{t})=P(S_{t+1}|S_{t},A_{t}) \\tag{34.10}$$\n",
    "\n",
    "$$P(A_{t+1}|S_0, A_0,..., S_{t}, A_{t},S_{t+1})=Π(A_{t+1}|S_{t+1}) \\tag{34.11} $$\n",
    "\n",
    "**第三步：合并状态-动作对的表达式**\n",
    "\n",
    "$$P(S_{t+1}|S_{t},A_{t}) \\times Π(A_{t+1}|S_{t+1})=P(S_{t+1},A_{t+1}|S_{t},A_{t}) \\tag{34.12}$$\n",
    "\n",
    "根据之前的表达式可以得到：\n",
    "\n",
    "$$P(S_{t+1},A_{t+1}|S_0,A_0,...,S_{t},A_{t})=P(S_{t+1}|S_{t},A_{t}) \\times P(A_{t+1}|S_{t+1}) \\tag{34.13}$$\n",
    "\n",
    "因此，状态-动作对满足马尔可夫性。\n",
    "\n",
    ">马尔可夫性质指的本质是：给定当前状态-动作对，未来状态的概率分布与过去历史条件独立。换言之，当前状态包含了关于未来演化所需的全部信息，因此它[构成过去历史的充分统计量](https://en.wikipedia.org/wiki/Markov_decision_process)。\n",
    "\n",
    "参考资料：\n",
    "1. [马尔可夫决策](https://en.wikipedia.org/wiki/Markov_decision_process)\n",
    "2. [结合贝叶斯网络理解MDP](https://www.cs.cmu.edu/~mgormley/courses/10601-s23//slides/lecture21-mdp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fcb90",
   "metadata": {},
   "source": [
    "## 习题34.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2a2ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;证明两种价值函数之间的关系(34.8)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cbb2c",
   "metadata": {},
   "source": [
    "**解答：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266fc48",
   "metadata": {},
   "source": [
    "**解答思路：**\n",
    "\n",
    "1. 理解状态、动作价值函数的定义\n",
    "2. 应用期望公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42450a35",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "**第一步：写出状态、动作价值函数的定义**\n",
    "\n",
    "其中 $G_t$ 表示回报函数 $\\sum_{t'=t}^{T-1} R(S_{t'},A_{t'})$ ：\n",
    "\n",
    "1. 状态价值函数定义：\n",
    "\n",
    "$$V_{Π}(S_t)=E_{Π}[G_t|S_t] \\tag{34.14}$$\n",
    "\n",
    "表示在策略 $Π$ 中，从状态 $s$ 开始的期望回报。\n",
    "\n",
    "2. 动作价值函数定义：\n",
    "\n",
    "$$Q_{Π}(S_t,A_t)=E_{Π}[G_t|S_t,A_t] \\tag{34.15}$$\n",
    "\n",
    "表示在状态 $s$ 先采用动作 $a$ ，再按照策略 $Π$ 执行的期望回报。\n",
    "\n",
    "**第二步：写出 $V_{Π}(s)$ 的展开式**\n",
    "\n",
    "在状态 $S_t$ 下，当前动作 $A_t$ 由策略 $Π$ 随机采样，因此**回报 $G_t$ 的不确定性可以通过对动作进行边缘化来刻画**，这里使用条件期望的迭代公式得到（34.16）：\n",
    "\n",
    "$$E[X|Y]=E[E(X|Y,Z)|Y]$$\n",
    "\n",
    "$$V_{Π}(S_t)=E_{Π}[G_t|S_t]=E_{Π(A_t|S_t)}[E_{Π}[G_t|S_t,A_t]|S_t] \\tag{34.16}$$\n",
    "\n",
    ">在强化学习中状态 $S_t$  、动作 $A_t$ 、策略 $Π$ 均为是随机变量，因此根据动作、状态得到的回报 $G_t$ 也是随机变量。\n",
    "\n",
    "**第三步：思考式子（34.15）、（34.16）的关系**\n",
    "\n",
    "把式子（34.15）代入 （34.16）中，可以得到：\n",
    "\n",
    "$$V_{Π}(S_t)=E_{Π(A_t|S_t)}[E[G_t|S_t,A_t]|S_t]=E_{Π(A_t|S_t)}[Q_{Π}(S_t,A_t)|S_t] \\tag{34.17}$$\n",
    "\n",
    "参考资料：\n",
    "1. [条件期望](https://en.wikipedia.org/wiki/Conditional_expectation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
